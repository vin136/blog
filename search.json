[
  {
    "objectID": "posts/experimentation/index.html",
    "href": "posts/experimentation/index.html",
    "title": "Experimentation in the Wild : Tools",
    "section": "",
    "text": "1. Hypothesis testing\nHere the business is all about studying the effect of treatments. I skip the motivation and other formal sections and cut right through the meat. Also we make no comments on experiment design. We pretend someone else administered the experiment and gave us the data. We’ll learn tools to analyze this data and make sound conclusions.\nStatistical Significance: Calculate how often pure chance can let you observe the effect as large as seen in the observed data, in the absence of any real effect(null hypothesis).\nBased off this value, we decide to whether reject or accet null hypothesis.\nProblem\nLet’s say we made a code change to improve the latency of the system. Here are the readings before and after. Avg. latency after the change is 20.33 ms while before is 25ms. Is this difference significant enough to conclude that our change improved the latency ?\nbefor: [30,25,18]\nafter : [20,21,22]\nApproach:\n\nDecide what do we mean by significant enough ?\n\n\ntest-statistic T: A function of observed data whose value represents the result of the test.\n\nLet’s use difference of means. (T = mean(before) - mean(after), the larger the T, the stronger the effect)\n\nBe precise about what do you mean by null hypothesis\n\nLet \\(\\mu_0\\), \\(\\mu_1\\) be the mean latency before and after the code change.\n\\(H_0\\) (null hypothesis) : \\(\\mu_0\\) = \\(\\mu_1\\)\nNow the alternative of interest(what we anticipate)\n\\(H_1\\) : \\(\\mu_0\\) &gt; \\(\\mu_1\\)\n\nDecide on the significance and conclude\n\nP-value: Probability that a chance alone would produce a test statistic as extreme as the one observed if the null hypothesis is true.\nnull distribution: The distribution of the test statistic if the null-hypothesis is true.\nThe choice of P-value is upto us. Suppose we choose 0.02, that means chance alone could produce a test statistic (24.3-21 =3.3ms)as extreme as the observed one 2 out of 100 times. This assumes an underlying generative process of the data(under null hypothesis). If the observed\n\nimport numpy as np\n\n\nb = np.array([30,25,18])\na = np.array([20,21,22])\n\nthresh = 0.02\n\nt = np.mean(b)-np.mean(a)\n\nSimulate the generative process under null. Given there is no difference in latency before and after the code change, our values after the change could be any of the \\(6 \\choose 3\\) possibilities from [30,25,18,20,21,22]. Let’s calulate the P-value of observing 3.3 or more just by pure chance.\n\nfrom itertools import permutations\nc = [30,25,18,20,21,22]\n#list of all possible permutations\nl = list(permutations(c))\nis_extreme = []\nfor perm in l:\n    pos_b = perm[:3]\n    pos_a = perm[3:]\n    pos_t = np.mean(pos_b)-np.mean(pos_a)\n    is_extreme.append(pos_t&gt;=t)\n\n\np_value = np.mean(is_extreme)\np_value\n\n0.25\n\n\nConclusion 25% of times,chance alone would have given us the observed latency improvement(or more). Since this value is greater than the threshold, we fail to reject the null. We’ll not belive that our code change significantly improved the latency times.\nNote: Sometimes we might not be able to genrate all possibilities, then we resort to sampling. or We could resort to approximations based on some assumptions.\nPermutation Testing\nPool the m + n values\nrepeat\ndraw a resample of size m without replacement use remaining n observations for the other sample calculate the test statistic\nuntil we have enough samples.\nCalculate the #times random statistic exceeds the observed. Multiply by 2 for a two-sided test.\nThis post covers all the tools needed in the practice.\n\n\nA/B Testing\nScenario: We have two or more treatments and we seek to measure which one of those is good on average.\nConcretely, let’s take two machine learning models that we have trained Model1 and Model2. Though we’ve tested them on our test set, we still don’t know how they’ll do on production environment.\n\ndef"
  },
  {
    "objectID": "posts/deeplrrevision/index.html",
    "href": "posts/deeplrrevision/index.html",
    "title": "Deep Learning - Quick Revision",
    "section": "",
    "text": "Entropy\nThe entropy of a discrete random variable X with distribution p over K states is defined by:\n\\[ H(X) = -\\sum_{i=1}^{K} p(i) \\log_{2}(p(i)) \\]\nCross-entropy\n\\[ H(p, q) = -\\sum_{i=1}^{K} p(i) \\log_{2}(q(i)) \\]\nKL divergence\n\\[ D_{KL}(p \\| q) = \\sum_{i=1}^{K} p(i) \\log_{2}\\left(\\frac{p(i)}{q(i)}\\right) \\]\nRelation:\n\\[ H(p, q) = H(P) + D_{KL}(p \\| q) \\]\nWhy do we use KL for knowledge distillation ?\nIn case of distillation, H(P) is the teacher’s entropy,where there is no updates/gradient, thus makes no sense to include it in loss formulation."
  },
  {
    "objectID": "posts/deeplrrevision/index.html#infotheory",
    "href": "posts/deeplrrevision/index.html#infotheory",
    "title": "Deep Learning - Quick Revision",
    "section": "",
    "text": "Entropy\nThe entropy of a discrete random variable X with distribution p over K states is defined by:\n\\[ H(X) = -\\sum_{i=1}^{K} p(i) \\log_{2}(p(i)) \\]\nCross-entropy\n\\[ H(p, q) = -\\sum_{i=1}^{K} p(i) \\log_{2}(q(i)) \\]\nKL divergence\n\\[ D_{KL}(p \\| q) = \\sum_{i=1}^{K} p(i) \\log_{2}\\left(\\frac{p(i)}{q(i)}\\right) \\]\nRelation:\n\\[ H(p, q) = H(P) + D_{KL}(p \\| q) \\]\nWhy do we use KL for knowledge distillation ?\nIn case of distillation, H(P) is the teacher’s entropy,where there is no updates/gradient, thus makes no sense to include it in loss formulation."
  },
  {
    "objectID": "posts/deeplrrevision/index.html#general-techniques",
    "href": "posts/deeplrrevision/index.html#general-techniques",
    "title": "Deep Learning - Quick Revision",
    "section": "General techniques",
    "text": "General techniques\n\nWhy random initialization ?\n\n2. How to deal with exploding gradient ?\nClip gradients\n\nHow to deal with vanishing gradient ?\n\n\nBatchnorm/layer norm etc: Ensure activations are always in desired range. Say when using sigmoid/tanh, if the activations are closer to 0,we’ll have good(close to 1) gradient, thus no vanishing.\nArchitecture : modify it to have additive updates. (Residual connections)\nModify activations:\n- Relu: `pros`: no saturation `cons`: Dead lelu, if weights are large negative,some neurons turn off and never get updated. UPDATES: LEAKY RELU,ELU etc\nCareful initialization\nXavier Initialization : control the variance of both activations and the gradient.(just a heuristic)\n- Assumes no non-linearity, and weights,inputs are independent, with mean zero.\n- Keep the output variance same as input\n\n\n\nWays to deal with overfitting ?\n\n\nWeight-decay\nDropout\n\nDropout : Activations shouldn’t depend on any one of the weights. Randomly drop some weights during training.(introduce noise in an unbiased way) Two ways to implement:\n1.Training: preserve the weights by dividing active ones by 1-p(prob of active).\n\n\n\n1.Test: Just not use dropout\n\n\n\n- 2.Training: No adjustmet\n- 2.Test: remove dropout,Multiply weights by (1-p)"
  },
  {
    "objectID": "posts/deeplrrevision/index.html#architecture-abstractions",
    "href": "posts/deeplrrevision/index.html#architecture-abstractions",
    "title": "Deep Learning - Quick Revision",
    "section": "Architecture abstractions",
    "text": "Architecture abstractions\nVarious architectures are our ways of encoding our inductive biases.\n\ntranslational invariance: Output is same irrespective of the location in cnn’s. Achieved by pooling\ntranslational equivariance: shift+conv = conv+shift\n\nNatural signals share 3 properties\nLocality: easier to predict future using recent past than earlier. =&gt; Sparsity\nStationarity: same patterns repeat again and again. =&gt; Weight sharing.\nCompositionality: =&gt; Deeper networks."
  },
  {
    "objectID": "posts/deeplrrevision/index.html#optimization",
    "href": "posts/deeplrrevision/index.html#optimization",
    "title": "Deep Learning - Quick Revision",
    "section": "Optimization",
    "text": "Optimization"
  },
  {
    "objectID": "posts/deeplrrevision/index.html#optimization-refer-cowans-notes",
    "href": "posts/deeplrrevision/index.html#optimization-refer-cowans-notes",
    "title": "Deep Learning - Quick Revision",
    "section": "Optimization (refer cowan’s notes)",
    "text": "Optimization (refer cowan’s notes)\nGradient descent =&gt; Taylor series expansion : F(x) ≈ F(xt) + F′(xt)(x − xt) + 1/2F′′(xt)(x − xt)^2.\nDirection : This tells under second order approximation =&gt; move opposite to direction of grad =&gt; func value decreases.\nAmount : How much to move =&gt; roughly on the order of inverse second derivative. More curvature= move slowly.\nMomentum: At any point continue moving a small part in the direction previously moved.\n\\[x_{t+1} = x_t − α_t∇F (x_t) + β_t(x_{t} − x_{t−1})\\]\nIntuition: Using the information of both current and previous gradient = approximating second order methods/ proxy estimation of curvature.\nWhat’s Good Step-Size ?\n\ngenerally it should be inversely proprotional to the curvature\n\nADAGRAD : If the past updates for a component has been smaller =&gt; less curvature =&gt; can use higher learning rate. Method: Refer\nFlaw: because the accumulated gradients Git are perpetually increasing by positive amounts, the stepsizes will be decreasing to zero. If this happens to quickly, then the algorithm effectively ‘freezes’ and is unable to improve.\nRMS PROP: Instead of taking sum of past gradients along a direction, we can take decaying average METHOD\nADAM: RMSPROP+ MOMENTUM\ndirection =&gt; momentum\nlearning rate for each direction =&gt; rmsprop\nmomentum+rmsprop = adam."
  },
  {
    "objectID": "posts/deeplrrevision/index.html#transformers",
    "href": "posts/deeplrrevision/index.html#transformers",
    "title": "Deep Learning - Quick Revision",
    "section": "Transformers",
    "text": "Transformers"
  },
  {
    "objectID": "posts/collected-thoughts-1/index.html",
    "href": "posts/collected-thoughts-1/index.html",
    "title": "Think on these #1",
    "section": "",
    "text": "Only tools we are in complete charge are our body, mind. All life happens through them. It pays to think in terms of this simple model. Much of the unnecessary suffering arises out of confusion with other minds. We have no access to the consciousness of other humans. So we cannot control their likes, dislikes, affection towards you. But, we seem to be in constant conflict with this reality. Our relation to the world occurs in pairs of action and expectation. But, can there be action without a desire or exception?"
  },
  {
    "objectID": "posts/collected-thoughts-1/index.html#action",
    "href": "posts/collected-thoughts-1/index.html#action",
    "title": "Think on these #1",
    "section": "",
    "text": "Only tools we are in complete charge are our body, mind. All life happens through them. It pays to think in terms of this simple model. Much of the unnecessary suffering arises out of confusion with other minds. We have no access to the consciousness of other humans. So we cannot control their likes, dislikes, affection towards you. But, we seem to be in constant conflict with this reality. Our relation to the world occurs in pairs of action and expectation. But, can there be action without a desire or exception?"
  },
  {
    "objectID": "posts/collected-thoughts-1/index.html#mental-states",
    "href": "posts/collected-thoughts-1/index.html#mental-states",
    "title": "Think on these #1",
    "section": "Mental states",
    "text": "Mental states\nThe joy of achievement, the pain of losing a loved one are fundamentally different mental states, invoked under certain conditions. The desired mental states we all seek are similar. In life, we try to line it up with certain mental states while avoiding others. This is the origin of all our desires. So it seems pertinent to find out the controller of these states? From where do they arise ? Does pre-conditions matter to get into a certain state ?"
  },
  {
    "objectID": "posts/collected-thoughts-1/index.html#move",
    "href": "posts/collected-thoughts-1/index.html#move",
    "title": "Think on these #1",
    "section": "Move",
    "text": "Move\nLife is in motion. Stagnation is death. Take action. That’s the only way to interact and influence. Time is ticking away. There is nothing to be preserved. So move into every moment - without baggage and drama."
  },
  {
    "objectID": "posts/collected-thoughts-1/index.html#now",
    "href": "posts/collected-thoughts-1/index.html#now",
    "title": "Think on these #1",
    "section": "Now",
    "text": "Now\nIt’s a strange realization. There is only now. All our dreams, if realized will have to dawn on some now. There is no escape from now. All the sensations that we derive from indulging in work, thoughts will be experienced now. So the scope of our life depends on our capacity to experience now. It’s a strange realization\nLife is happening now. Whether I plan for future, muse on my past or feel the burning pain of my wound , It all happens right now and that’s all I got. A moment arises within the cessation of the previous one. There is no way to latch. In thinking about the future – all my sensations whether the excitement or the dread happens in the present. So it follows whether can I keep myself joyful right now – without the necessity of a person, thing, belief or ideology. That would be cool."
  },
  {
    "objectID": "posts/collected-thoughts-1/index.html#inefficiency",
    "href": "posts/collected-thoughts-1/index.html#inefficiency",
    "title": "Think on these #1",
    "section": "Inefficiency",
    "text": "Inefficiency\nLife is mostly inefficient. We end up doing things that in retrospect offer no value. We regret. Yet, this trial and error process is the only way to discover. All our expectations, thoughts, longings are based on our present level of awareness. We cannot act like our future selves.Realizing this is self-empowering."
  },
  {
    "objectID": "posts/collected-thoughts-1/index.html#love",
    "href": "posts/collected-thoughts-1/index.html#love",
    "title": "Think on these #1",
    "section": "Love",
    "text": "Love\nA heart brimming with love and a radiant body are necessary to experience life at its fullest. Love doesn’t require anything from outside one’s agency. It doesn’t demand people to be less mean nor for us to constantly look for justification in their actions. It doesn’t arise out of any expectation from oneself or others. Whoever is around you, the habit of having good intentions for their life creates immediate wellbeing. Giving your best to the task, say in playing football for your team, might involve strategizing against your opponent. But, nowhere does it require you to change your intentions. It only becomes a zero-sum game when you rest your source of joy outside yourself – like in winning a game or getting a promotion. Then, inevitably subtle forms of hate arise helping us maneuver through dissatisfaction. But, this leaves us incapable of Joy."
  },
  {
    "objectID": "posts/collected-thoughts-1/index.html#on-belief",
    "href": "posts/collected-thoughts-1/index.html#on-belief",
    "title": "Think on these #1",
    "section": "On Belief",
    "text": "On Belief\nBelief is the easy thing. It demands no cognitive load. Decisions are made almost unconsciously. Doubt takes deliberation. Belief is common. It brings together people and offers safety. Groups welcome or ostracize individuals based on common beliefs.Evolution further selects these individuals. The desire to be identified with a group is an ever operating natural force. This raises the odds of our survival in the wild. While our well-being lurks on the fine balance of Doubt and Belief."
  },
  {
    "objectID": "posts/DeepRl/index.html",
    "href": "posts/DeepRl/index.html",
    "title": "Deep Reinforcement Learning - Theory",
    "section": "",
    "text": "I plan on to write series of posts explaining key ideas in Modern Reinforcement Learning(RL). Current post covers basics of deep learning,followed by an introduction to RL Theory.(Thourougly proves two key algorithms for learning in RL setting - Value Iteration and Policy Iteration.). If you prefer a more practical introduction refer this.\nNote : This is not intended to be the first introduction to deep learning. Here I just provide a self-contained summary. Only hard prerequisite is to have good intuitions for matrix multiplication and the notion of taking a derivative. Otherwise refer to Essence of linear algebra and Calculus"
  },
  {
    "objectID": "posts/DeepRl/index.html#basics",
    "href": "posts/DeepRl/index.html#basics",
    "title": "Deep Reinforcement Learning - Theory",
    "section": "Basics",
    "text": "Basics\nMany deep learning models follow a simple recipe:\n1. Gather the data.\n2. Define learnable parameters. And specify how they will interact with the data.(architecture)\n3. Define a loss function to minimize.\n4. Adjust the parameters until satisfied.\n\nTrain a linear regression model using gradient-descent.\nHere we will see how we can perform all the above steps starting with the most barebones implementation. Note that the procedure outlined here is general purpose - meaning the way we adjust parameters is going to remain same irrecpective of the modality of the data, details of the loss function or the architecture.\nStep 1. Gather the data\nLet’s generate some fake data.Let’s assume that the data is coming from \\(y = 2*x1 - 4.2*x2 + 1 + noise(measurement error)\\). This can be more succintly represented in vector notation : \\[y = \\begin{bmatrix} x1 \\\\ x2 \\end{bmatrix} . \\begin{bmatrix} 2 \\\\ -4.2 \\end{bmatrix} + 1\\]\n\nimport numpy as np\ndef get_data(*params,const=None,rows=1000):\n    #number of features in the input\n    dim = len(params)\n    x = np.random.normal(0,0.3,(rows,dim))\n    y = x@np.array([params]).T\n    if const:\n        y += np.array([const])\n    return x,y\n\nx,y = get_data(2,-4.2,const=1)\n\nx.shape,y.shape\n\n((1000, 2), (1000, 1))\n\n\nStep 2. Define learnable parameters. And specify how they will interact with the data.(architecture)\nNow we aim to learn the right coefficients to approximate the data generation process. First let’s look at some code.\n\n# Start with a random guess that respects the sanctity of the data.i.e our inputs are of dimension 1000*2 \n# outputs are 1000*1. Multiplying inputs by a 2*1 matrix(weights) and adding a constant(bias) is the simplest way\n# to ensure an output of 1000*1. \n\n# initial guess\ninit_weights = np.array([[0.,-1.]]) #shape -&gt; 1*2\ninit_bias = np.array([0.])\n\n#expected output\ndef give_expected_output(inpt,weights,bias):\n    return ((inpt@weights.T) + bias)\n\nout = give_expected_output(x,init_weights,init_bias)#shape -&gt; 1000*1\n\ndef get_error(out,expected_out):\n    return np.mean((expected_out - out)**2)\n\nget_error(out,y) # IF WE CAN DRIVE THIS NUMBER DOWN TO ZERO VIA A GENERAL PURPOSE PROCESS,WE ARE GOOD TO GO\n\n2.121630476687219\n\n\n\ndef get_grads(weights,bias,x,y,loss_func='squared_loss'):\n    # Just taking the mathematical gradient as defined by the model.\n    if loss_func == 'squared_loss':\n        weights_grad = 2*np.mean((x@weights.T + bias - y)*weights)\n        bias_grad = 2*np.mean((x@weights.T + bias - y))\n    else:\n        print(\"Sorry I'm not yet scalable enough for arbitrary loss functions\")\n    return weights_grad,bias_grad\n        \n\n\ngrad_init_weights,grad_bias = get_grads(init_weights,init_bias,x,y,loss_func = 'squared_loss')\n\ndef learn(x,y,init_weights,init_bias,loss_func,lr=0.001,epochs=5000):\n    out = give_expected_output(x,init_weights,init_bias)\n    error = loss_func(out,y)\n    #print(f'initial error, epoch 0: {error}')\n    errors = [error]\n    pres_lr = lr\n    for i in range(epochs):\n        weight_grad,bias_grad = get_grads(init_weights,init_bias,x,y)\n        init_weights -= weight_grad*pres_lr\n        init_bias -= bias_grad*pres_lr\n        out = give_expected_output(x,init_weights,init_bias)\n        error = loss_func(out,y)\n        if np.mean(weight_grad)&lt;0.0001:\n            pres_lr = pres_lr*2\n        errors.append(error)\n    return errors,init_weights,init_bias\n        \n\n\nerrors,final_weights,final_bias = learn(x,y,init_weights,init_bias,get_error)\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.plot(errors)\n\n\n\n\n\nfinal_weights,final_bias\n\n(array([[-0.82604061, -1.82604061]]), array([0.97582166]))\n\n\nIn the above code what happens if ‘lr’ is not dynamically adjusted ? Now, imagine an arbitrary architecture (interaction between parameters and data). Can we have a general purpose get_grads function that efficiently evaluates the gradient for this network ?. For many such practical concerns, I suggest going through this free course from fast.ai. Modern libraries like Pytorch provide simple abstractions to ignore all such details. Now some general intuitions towards coming up with architectures for learning more complex functions.\n\n\nAdding Inductive biases - convolutions and recurrent networks.\nIn the below feedforward network each neuron is connected to all the neurons in the previous layer. No inductive biases in the connectivity.\n\n\n\nFeedForward\n\n\nLocality : For almost all natural signals it is easier to predict the future using recent past compared to any earlier versions. Locality allows for the sparsity of weights. We can put faraway weights to zero. In the below figure the 15 weights of the first layer is reduced to 9. It’s also important to be aware of the concept of receptive field(RF). RF of layer a w.r.t b is simply the number of neurons in a that influence the outputs of layer b.\n\nStationarity : Same patterns are repeated again and again. We don’t need connections from the inputs far down. Stationarity implies weight sharing.\n\nCompositionality: There are hierarchies. Letters make up words,words make up sentences and so on. Compisitionality implies deeper networks.\nNow We will see how popular building blocks like CNN’S and RNN’S leverage these properties. CNN’s are a linear layer with lots of weight sharing and sparsity. RNN’s just use weight sharing but BPTT takes the locality into account.\nCNN = linear layer + weight sharing + sparsity.\nConsider convolving over a 4 * 4 inputs with a 3 * 3 kernel with a unit stride as presented below. \nIf I stack the 2-d input into a 1-d vector by unrolling left to right and top to bottom, the convolution can be represented as a matrix multiplication. \nThe zeros along the columns encode locality while the replication of same weights along the rows account for stationarity. If the above properties doesn’t make sense for your input , then CNN’s aren’t the right choice.\nRNN = linear layer + weight sharing + BPTT(Back-prop through time)\n\nRNN’s are used for sequence data. In the above figure the arrow indicates matrix multiplication. The hidden state h(t) at time t is equal to Affine_transform(x(t)) + Affine_transform(h(t-1)).(Note: Affine_transform refers to matrix multiplication). The following code will makes it all clear.\nConsider the following sequence :\n’Hey Jude, don’t make it bad.\nTake a sad song and make it better.\nRemember to let her into your heart,\nThen you can start to make it better. ’\nNow we would want to classify it into either positive or negative sentiment. Ideally we would have a collection of such sequences with their corresponding labels. Here note that the number of words in each sequence need not be same. A naive approach would be to string all the words in a sequence to a single column,and run it through a fullyconnected network.(variable sequence length still poses a problem.) Let’s see how an RNN can accomplish this with much less parameters.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nclass RNN(nn.Module):\n    def __init__(self,i_sz,h_sz,out_sz):\n        super().__init__()\n        self.i_sz = i_sz\n        self.h_sz = h_sz\n        self.out_sz = out_sz\n        self.input_hidden = nn.Linear(self.i_sz,self.h_sz)\n        self.hidden_hidden = nn.Linear(self.h_sz,self.h_sz)\n        self.hidden_output = nn.Linear(self.h_sz,self.out_sz)\n    def forward(self,x):\n        h = 0\n        bs,seq_len,emb_sz = x.shape\n        for i in range(seq_len):\n            # This just adds the hidden representation which is a function of all the words fed until t-1 to the \n            #word at t\n            h = h + self.input_hidden(x[:,i,:])\n            # Stores the hidden representation for next  word in seq.\n            h = F.relu(self.hidden_hidden(h))\n        return self.hidden_output(h)\n        \n\n\nbs = 101\nseq_len = 10\nvector_len = 100\nseq_1 = torch.rand(bs,seq_len,vector_len)\nrnn = RNN(i_sz = vector_len,h_sz=20,out_sz=1)\nout = rnn(seq_1)\n\nout.shape\n\ntorch.Size([101, 1])\n\n\nThis can be seen as a two layer network.The last layer converts h_sz to the output dimension, while the first layer maps i_sz to h_sz. But, the first layer only uses weight matrices of size 100 * 20,20 * 20. Totalling of around 2400 parameters. Our naive version would have seq_len * i_sz * h parameters.(around 20000). Moreover, In RNN number of paramenters is independent of sequence length.\nThe above model is just an instantiation of weight sharing for sequential data. We haven’t still leveraged the locality aspect (sparsity).\nMoreover,eventhough we only have 3 different weight matrices,the actual number of layers is proportional to the size of the for loop. Aside from being very slow and memory intensive, gradients of loss w.r.t initial operations( i = 0) very unlikely to be stable.(According to the chain rule of derivatives the gradient of loss w.r.t first matrix multiplication would involve multiplying atleast seq_len of partial derivatives. The resultant can easily explode or vanish.) What if we only take gradients for the last n operations. This is also called Truncated BPTT. Here’s the modified forward function.\n\ndef forward(self,x):\n        h = 0\n        bs,seq_len,emb_sz = x.shape\n        for i in range(seq_len):\n            # This just adds the hidden representation which is a function of all the words fed until t-1 to the \n            #word at t\n            h = h + self.input_hidden(x[:,i,:])\n            # Stores the hidden representation for next  word in seq.\n            h = F.relu(self.hidden_hidden(h))\n            if i%3 == 0:\n                h = h.detach()\n        return self.hidden_output(h)\n\nThis just flushes the memory for the backward pass after every 3 steps. Aside from solving obvious practical problems,this also has regularizing effects. We are implicitly encoding our bias - you need not look past the last 3 points in the sequence - a.k.a locality.\n\n\nSelf-Attention\nConsider a sequence of vectors (\\(x_1\\),\\(x_2\\)..\\(x_n\\)). It helps to imagine them as vectors corresponding to sequence of words. If you can bear with me,the following operation converts them into another sequence of vectors (\\(y_1\\),\\(y_2\\)..\\(y_n\\)) of same dimension.\n\n# Two column vectors(each with dimension of 3*1)\nimport torch\nimport torch.tensor as tensor\nX = torch.cat([tensor([[1.],[2.],[3.]]),tensor([[4.],[5.],[6.]]),tensor([[7.],[8.],[9.]]),tensor([[10.],[11.],[12.]])],dim=-1)\nmatrix = torch.rand(4,4) # a random matrix\nY = X@matrix\nX.shape,matrix.shape,Y.shape\n\n(torch.Size([3, 4]), torch.Size([4, 4]), torch.Size([3, 4]))\n\n\nNow,let’s generate the same V with a fancier set of operations.\n\ntemp = X.T@X\nY_new = X@temp.T\nY_new.shape\n\ntorch.Size([3, 4])\n\n\nHere temp.T is acting as matrix. This also removes the need for additional initialization. This operation also lends to the following intution:\n\ntemp is the dot product of each column vector in U with all the vector within it. The captures the measure of similarity between the vectors.\nY_new is just a linear combination of U with the corresponding weights from the temp.\nIn other words, \\(y_i =\\sum_{j} w_{ij}.x_{j}\\) where w’s are taken frow the rows of temp.\n\nIn the above figure each vector \\(x_{i}\\) is used three times. Let’s take \\(x_{2}\\) for illustration:\n\nTo get \\(w_{22}\\)\nSimilarly to get weight’s required for all the other outputs \\(y_1\\),\\(y_3\\) and \\(y_4\\)\n\\(x_2\\) is also used in linear weighting with w’s to get \\(y_2\\)\n\nBut, In the whole compution we are not learning any weights. Everything is being generated from the input. We can introduce three different set of x’s for each of the above operations. Let’s initialize three square matrices \\(W_k\\),\\(W_q\\),\\(W_v\\), each of size (4,4).\n\nw_k,w_q,w_v = torch.rand(4,4),torch.rand(4,4),torch.rand(4,4) # learnable parameters\n\nkeys,queries,values = X@w_k,X@w_q,X@w_v\n# This is the naming convention used in the literature.\ntemp = queries.T@keys\nY_new =values@temp.T\nY_new.shape\n\ntorch.Size([3, 4])\n\n\nThat’s it. Self-attention refers to performing above operations. We additionally normalize the weights in the temp with softmax. Further, We can also use sets of matrices (\\(W_k\\),\\(W_q\\),\\(W_v\\)),essentially replicationg self-attention with different weight matrices. The resulting outputs can be concatenated and be passed through a linear layer to get back the orginal dimension.(This is called multi-head attention)\n\nimport torch.nn as nn\nimport math\nimport torch.nn.functional as F\n\n\nclass SelfAttention(nn.Module):\n    def __init__(self,feat_sz,n_heads=1):\n        super().__init__()\n        # for n_heads we need the corresponding number of weight matrices of size feat_sz*feat_sz to get new\n        #set of (keys,queries,values),Computationally this can be fused inside a single linear operation.\n        self.heads = n_heads\n        self.get_keys = nn.Linear(feat_sz,feat_sz*n_heads,bias=False)\n        self.get_queries = nn.Linear(feat_sz,feat_sz*n_heads,bias=False)\n        self.get_values = nn.Linear(feat_sz,feat_sz*n_heads,bias=False)\n        self.comb_heads = nn.Linear(n_heads*feat_sz,feat_sz)\n        \n    def forward(self,x):\n        # typically data is fed with features along the `columns`.\n        bs,n_seq,feat_sz = x.size()\n        keys = self.get_keys(x).view(bs,n_seq,self.heads,feat_sz)\n        queries = self.get_queries(x).view(bs,n_seq,self.heads,feat_sz)\n        values = self.get_values(x).view(bs,n_seq,self.heads,feat_sz)\n        # `torch.bmm` performs matrix multiplication for a given batch.It is efficient to squeeze n_heads along\n        # with batches and perform the calculation at once.\n        keys = keys.transpose(1,2).contiguous().view(bs*self.heads,n_seq,feat_sz)\n        queries = queries.transpose(1,2).contiguous().view(bs*self.heads,n_seq,feat_sz)\n        values = values.transpose(1,2).contiguous().view(bs*self.heads,n_seq,feat_sz)\n        dot = torch.bmm(queries,keys.transpose(1,2))\n        #Rescaling the elements to control the scale\n        dot = dot/math.sqrt(feat_sz)\n        dot = F.softmax(dot,dim=2)\n        out = torch.bmm(dot,values).view(bs,self.heads,n_seq,feat_sz)\n        #reshaping\n        out = out.transpose(1,2).contiguous().view(bs,n_seq,self.heads*feat_sz)\n        # passing through a linear layer to combine all the heads\n        out = self.comb_heads(out)# gives bs,n_seq,n_heads\n        \n        return out\n\n\ninput_ = X.unsqueeze(0).transpose(1,2) # input with features arranged in columns(shape = [1, 4, 3])\n\nsa = SelfAttention(feat_sz=3)\nY = sa(input_)# simple attention\n\nmha = SelfAttention(feat_sz=3,n_heads=6)\nY_6 = sa(input_)\n\nY.shape,Y_6.shape\n\n(torch.Size([1, 4, 3]), torch.Size([1, 4, 3]))\n\n\n\nNote: Attention is first introduced to deal with sequences in the context of natural language. But, nothing in the above implementation handles order. It just maps a set of vectors to another. To enforce order, we can simply add a position vector to our inputs.\n\nAbove ideas mark the end of the theoretical minimum. These will be needed only when you are trying to solve any of the interesting applications discussed below with RL."
  },
  {
    "objectID": "posts/DeepRl/index.html#deep-rl---theory",
    "href": "posts/DeepRl/index.html#deep-rl---theory",
    "title": "Deep Reinforcement Learning - Theory",
    "section": "Deep RL - Theory",
    "text": "Deep RL - Theory\n\nMotivation\nDeep Learning allows for learning generalizable mappings between input and output. In supervised setting we are given a fixed dataset D = {\\((x_{i},y_{i})\\)},and are tasked to predict \\(y_{i}\\) using \\(x_{i}\\). Thus, we know the groud truth for all input data. This let’s us formulate a loss that reflects our dissatisfaction with the predicted outputs and optimize over it. But, consider how we humans learn to perform any new task ? The dataset and the learning signal comes sequentially and is also dependent on our actions. We don’t have prepared datasets in real life. We learn from experience. RL deals with learning under this natural setting. The key difference here is that the dataset is not constant, It changes everytime, contingent on your actions and the stochasticity in the environment. To make this clear, consider learning to play a video game. The pixels (call it state) and the score you receive ( call it reward) cannot be predetermined until you actually go through the experience. Here the dataset D = {\\((state_{i},reward_{i})\\)} is not same everytime you play the game. RL provides a formalism for learning optimal decision making. This will become clear when we see some concrete examples and code. But combing these techniques with Neural networks has given us general algorithms that learn to play atari games, beat world champions at Go and train robots to learn simple tasks. \n\n\nProblem\nConsider the Following optimization problem. Find the shortest path from state \\(s_0\\) to goal \\(g\\),where the edges indicate the cost/distance ?\n\nHere taking a greedy approach will fail as actions will have long-term consequences. Solving the above problem efficiently requires realizing that the distance to any node along the shortest path from source to destination is also shortest path. In the above example the shortest path to g should either be through d or f.(one of the incoming edges). Let the shortest distance from a node s to g be given by \\(v^{*}(s)\\). Then the last edge of the shortest path should come from evaluating \\(v^{*}(f))\\) and \\(v^{*}(d)\\), where \\(v^{*}(f) = 1 + v^{*}(g)\\) and \\(v^{*}(d) = min(3+v^{*}(g),1+v^{*}(f))\\). This backtracking or dynamic programming approach of finding one edge at a time is illustrated below.\n\nNow, for each action we take, let us also add a transition probability that defines the likelihood of ending up in any of the available states given the action. Here, for states c and e we added some randomness. This will let us model more realistic scenarios. Consider an RL agent driving your car. The consequences of any action (eg: Turning the steering to the right given the visual view of the road.) can only be modeled probabilistically.\n\nHere by weighting w.r.t the transition probabilities we can recover \\(v^{*}()\\) for all states. Optimal policy is again achieved by acting greedily w.r.t \\(v^{*}()\\). For example , \\(v^{*}(c) = min(4+0.3*v^{*}(e)+0.7*v^{*}(d),2+v^{*}(e))\\). In RL, we call this Bellmann Equation.\n\n\nModern Deep RL deals with solving this stochastic version of the problem when the transition probabilities are not available and the number of possible states is very large. Consider the following video game playing scenario. The pixels on the screen at any timestamp can be taken as state. The game rules provides list of possible actions and the corresponding reward(increase in score).\n\nBy the end of this module we will develop all the machinery to understand the algorithms that learn optimal(or good) policies for this general case.\n\n\nBellmann Equation and MDP’s\nIn this section we will devolop some theory. First let’s define an object called Markov Decision Process (MDP),given by the tuple of \\((S,A,P,R,\\gamma)\\). Here discounting factor \\(\\gamma\\) is largely a mathematical convenience as it helps to bound the total reward. Any reward \\(r_t\\) received at timestamp t is multiplied by \\(\\gamma^{t-1}\\), that is we prefer immediate rewards over faraway ones. The term Markov refers to the fact that given present state s and action taken from there a, next state is independent of the past trajectory. Consider the example of stochastic shortest path but with the goal state g infinitely far away. In that setting we want to maximize the average reward we will get starting from any state.\nThe only knob agent has is policy \\(\\pi : S \\rightarrow A\\).(which of the possible actions to take). The environment dynamics (P and R) are not under agent’s control. For simplicity,let’s assume that rewards are bounded and positive. Let’s say that the agent has a policy \\(\\pi\\) and starts to intreract with the environment. The value function v(s) refers to the average reward starting from state s and following policy \\(\\pi\\). Then v(s) for all the states satisfies a recursive definition as shown below. \nThus finding \\(v^{\\pi}(s)\\) amounts to solving system of linear equation or in matrix notation finding the inverse of a matrix. But, we still need the proof for the existence of the inverse for \\(I - \\gamma.P^{\\pi}\\). Before going further, it’s important to thoroughly understand the bellmann equation : \\(V^{\\pi} = R^{\\pi} + \\gamma.P^{\\pi}.V^{\\pi}\\). The deriviation involves observing the fact that once the agent takes initial action from state s, the transition probability will dictate it’s next state s'. From there, the average reward by definition is given by v(s'), leaving a recursive definition.\n\nBelow i have give the proof for the existence of inverse. If we recall, a matrix A(\\(n * n\\)) multiplied by a column vector X (\\(n * 1\\)) merely takes a linear combination of all the column vectors in A. Existence of inverse to A means that there doesnot exist a non-zero vector X, that can collapse A to a null vector. Using this fact and the traingular inequality on \\(I - \\gamma.P^{\\pi}\\) completes the proof.\n\n\n\nSearch for optimal State Values.\nNow let’s define \\(v^{*}(s)\\) as the maximum expected reward that we can get from state s under any policy. Note that the above equation is defined on a particular policy or when the action from each state is fixed or if state transition probabilities are independent of action taken. Once we are able to extract these values, optimal policy becomes obvious. Value Iteration methods try to apprimate this \\(V^{*}(s)\\). They start with some arbitrary function like \\(f(s) = 0 \\forall s\\) and iteratively bring \\(f\\) closer to \\(V^{*}\\). Let’s also define \\(Q^{\\pi}(s,a)\\) as the expected reward under the policy \\(\\pi\\),when we take action a at state s and subsequently sample actions according to \\(\\pi\\). Here \\(\\pi\\) is just a function that takes state s and action a and returns the corresponding probability a for taking that action. Similarly \\(Q^{*}(s,a)\\) is also defined as the maximum Q that can be achieved under any policy. We often want \\(Q^{*}\\) values over \\(V^{*}\\) as simply choosing greedily w.r.t \\(Q^{*}\\) gives the optimal policy. To see the advantage more clearly, Imagine an oracle that can give you \\(V^{*}\\) for any s. Now, the agent starts at state s and is looking to take optimal action. It first has to take all possible actions from that state to see where it would end up. And for each subsequent state s' , it has to query the oracle to get \\(V^{*}(s')\\). Only after this we can determine the best action from initial state s as \\(max_{a\\in A}(r_1 + v^{*}(s'))\\). But having \\(Q^{*}\\) values let’s us choose this action directly by evaluating \\(Q_{a\\in A}^{*}(s,a)\\). There exists a proof that for discounted infinite horizon MDP’s there exists a stationary and deterministic optimal policy for all states simultaneously. Let’s call this \\(\\pi^{*}\\). It is easy to see that both \\(V^{*}\\) and \\(Q^{*}\\) will also satisfy a similar recursive relation called Bellmann Optimality equations."
  },
  {
    "objectID": "posts/DeepRl/index.html#value-iteration",
    "href": "posts/DeepRl/index.html#value-iteration",
    "title": "Deep Reinforcement Learning - Theory",
    "section": "Value Iteration",
    "text": "Value Iteration\nTill now we have seen how we can solve for V for all states given a policy. But, Our objective is to find V* - the maximum return that can be achieved under any policy. The claim is that if we start with random values of Q and then do the following -\n\nAct greedily with respect to these values.\nUpdate these Q values using bellmann update rule.\nRepeat the process for some large number of times - H\n\nWill give us a policy \\(Q^{*,H}\\)(greedy policy w.r.t Q values after H Steps) that is close to true Optimal Policy. In what follows is the proof of the claim."
  },
  {
    "objectID": "posts/DeepRl/index.html#policy-iteration-pi",
    "href": "posts/DeepRl/index.html#policy-iteration-pi",
    "title": "Deep Reinforcement Learning - Theory",
    "section": "Policy Iteration (PI)",
    "text": "Policy Iteration (PI)\nWe also have an alternative algorithm that also converges to optimal policy called Policy Iteration.\nNote : PI strictly converges to optimal policy after some steps which is not true for VI as it only get’s closer but never quite equals.\nThe following derivation requires some explanation. First Here’s the claim of policy iteration algorithms.\n\nStart out with random policy.\nEvaluate \\(Q^{\\pi_{0}}\\) using bellmann update rule. (BOX 1).\nGreedy policy w.r.t to new Q values becomes your new policy.\n\nPerforming above steps for large number of iterations would give us Optimal Policy - Claim. (In fact the claim is more subtle as given by Policy Improvement Theorem.\nPolicy Improvement Theorem: As given in below slide suggests that after every round of PI, the new policy has higher(or equal) V compared to old for all states.\nThe proof involves manipulating \\(\\tau^{\\pi}\\) (bellmann operator). We start out by consicely writing down PI algorithm -\n\\(Q^{\\pi_{k}} = \\tau^{\\pi_{k}}.Q^{\\pi_{k}}\\)\nNote that bellmann optimality operator \\(\\tau\\)(involves taking max in next state) is defferent from bellmann operator \\(\\tau^{\\pi}\\). And substituting the latter with former would always result in a value higher or equal by definition. Since according to PI algorithm the new policy is the Greedy one w.r.t updated Q values, we have - \\(\\tau.Q^{\\pi_{k}} = \\tau^{\\pi_{k+1}}.Q^{\\pi_{k}}\\). Later by recursively expanding \\(Q^{\\pi_{k}}\\), we reach the fixed point of the bellmann operator.\n\n\nAlternative and more Intuitive proof:"
  },
  {
    "objectID": "posts/Linear-Algebra/index.html",
    "href": "posts/Linear-Algebra/index.html",
    "title": "Linear Algebra: 0.1",
    "section": "",
    "text": "Note: These are not meant to be used for self-teaching the subject. My objective is to collect the summary of all the ideas that a working Machine Learning Researcher should be aware of. Use this as a reference to check or deepen your understanding. At the start of each post I will provide an opinionated learning strategy to teach yourself the subject from scratch in the shortest possible time. In most cases the post is based off those resources, thus the credit is due to the orginal authors.\nPointers to learn from scratch (Go through these resources in the given order.)\nRead and workthrough the first 4 chapters of the book mml.\nWatch through this series of videos by 3Blue1Brown - Essence of linear algebra along the way.\nFor all practical purposes this should suffice. But, I would also suggest to read through Linear Algebra Done Right"
  },
  {
    "objectID": "posts/Linear-Algebra/index.html#idea-1-spanning-vectors",
    "href": "posts/Linear-Algebra/index.html#idea-1-spanning-vectors",
    "title": "Linear Algebra: 0.1",
    "section": "Idea 1: Spanning Vectors",
    "text": "Idea 1: Spanning Vectors\nBig Idea 1: All the elements of a vector space can be represented succintly as a linear combination of few a vectors. In other words,Irrespective of the vector space (geometric vectors,polynomials, functions etc), there exists a set of few vectors that can be used to generate all the elements of that set. This section builds up the material to prove this fact.\nSubspace\nThere exist a subset of the vector space that also satisfy the properties of the vector space. This will be more clear via examples below.\n\n\nTip: To check if a set forms a valid Subspace check if the elements of the set are closed under a.addition and b.zero element belongs to the set c. closed under scalar multiplication. Rest of the properties(distributivity,associativity etc) are anyways satisfied by definition(as these are subset of the vector space).\n\n\nNow it is sensible to ask whether by combining two subspaces(UNION,INTERSECTION) can we get another valid subspace. Convince yourself that the INTERSECTION of two subspaces is always a subspace which is not the case for UNION. If we define the notion sum of two subspaces as the collection all elements that can be written as sum of an element from each subspace, we note that this set is also a vector space thus a subspace of the original. Also this is the smallest subspace containing both the subspaces (just like how union of two sets in set theory is the smallest set containing both the sets).\n\n\n\nImportant: Understand the above claims before proceeding.\n\nSpan,basis and linear independence\nNote that give two vectors \\(v_1\\) and \\(v_2\\) the set \\(V = \\{\\lambda_1 \\hat v_1 + \\lambda_2 \\hat v_2: \\lambda_1,\\lambda_2 \\in C\\}\\) forms a subspace. Also note that if we have another vector \\(\\hat{v_3} = 2\\hat{v_1}\\) and define the new set \\(V'\\) as \\(V' = \\{\\lambda_1 \\hat v_1 + \\lambda_2 \\hat v_2 + \\lambda_3 \\hat v_3: \\lambda_1,\\lambda_2,\\lambda_3 \\in C\\}\\), we will still have \\(V = V'\\). Using \\(\\hat {v_3}\\) doesn’t give us any new vectors that is not already in \\(V\\). Let’s add two new words to our vocabulary Span and Linear Independence.\n\nBelow we see how span and linear independence are related.\n\nNow, we are certain that given a subspace spanned by \\(n\\) vectors, the subspace can also be spanned by a set of \\(k\\) linearly independent vectors where \\(k &lt;= n\\). What’s the size of the smallest \\(k\\) that can still span the space ?\nBasis: Set of the minimal number of vectors needed to generate all the elements of the subspace. It is easy to see that all such vectors should be linearly independent. (else it wont be the minimal set). Below examples will make the concept more concrete.\n\nIt can be trivially shown that the cardinality of the basis set(#number of vectors in the basis set) is constant, no matter which set of independent vectors are chosen. For example, in case of geometric vectors in \\(R^{n}\\), we could have chosen a differnt basis but it still has to have same count of vectors to span the same space. Thus we can call this cardinality the dimension of the vector space.\nNow given a spanning set, how to get the basis ?\n\nJust remove any vector that can be expressed as a linear combination of others until no more. At th end we are left with the basis set.\n\nSimalarly, We can build the basis set from a single vector(or a subset of the basis set) by adding vectors that cannot be expressed as a linear combination of the given set but belongs to the subspace.\nTo drive all these points home, let’s solve some problems dealing with span,dimension and linear dependence. Along the way we will also learn how to solve system of linear equations.\n\nSkill 1 : Basis and solution to system of linear equations\n1.Finding Solutions to system of linear equations.\nThis boils down to answering whether the right handside of the equation exist within the span of vectors defined by the equations as shown below.\n\nNow we know that any vector in \\(R^{4}\\) can be spanned by a set of 4 linearly independent vectors. Thus, if the above vectors are linearly independent we will have a unique solution,else none.(note that the uniqueness comes from the definition of basis - minimal set of linearly independent vectors.). We haven’t yet discussed how to find the coefficients - \\(x_1\\),\\(x_2\\),\\(x_3\\),\\(x_4\\). The techinque is called Gaussian Elimination in case the reader is not familiar check out this wiki entry.. Below is the solution.\n\nSolution Concept - Subtracting/adding one equation to the other does’nt change the solution of a system of equations. Gaussian Elimination Provides a systematic algorithm leveraging this fact. In the above question the last row reads as 0=1,thus we have no solution.\nNow Let’s see a case where there can be infinite solutions.Conceptually it means the set of vectors defined by the linear equations has redundant vectors.(more vectors than the dimensionality of the space). This will give us more than one way of reaching any point.\n\nNOTE: We have 5 vectors for a 4 dimensional space.\n\nSolution Concept - The idea is to reduce the vectors such that there is a 1 at a different slot for each of the vector,making reading off linearly independent vectors easy.(as any linear combination of the previous vectors cannot produce a value in the new slot.) In the above solution we note that \\(\\hat v_1\\),\\(\\hat v_3\\),\\(\\hat v_4\\) can produce 1’s at 1st,2nd and 3rd index repectively thus are linearly independent. This makes \\(\\hat v_2\\) and \\(\\hat v_5\\) redundant. Also observe that we can write \\(\\hat v_2\\) as linear combination of \\(\\hat v_1\\),\\(\\hat v_3\\),\\(\\hat v_4\\) implying we can have a zero. Similarly for \\(\\hat v_5\\). Below note makes this clear.\n\n\nImportant: If you have trouble solving above two questions, refer to the text mentioned at the beginning of the chapter. Here I only intend to show how the notion of span and independence translate to finding solutions to system of linear equations.\n\nBefore proceeding let’s consider one last problem.\n\nlet’s start by find the basis for \\(U_1\\) and \\(U_2\\). This will let us infer the space spanned by both the sets. Note here how finding basis set is helpful as it’s the simplest representation of the underlying vector space."
  },
  {
    "objectID": "posts/Linear-Algebra/index.html#summary",
    "href": "posts/Linear-Algebra/index.html#summary",
    "title": "Linear Algebra: 0.1",
    "section": "Summary",
    "text": "Summary\nWe understood and defined the notions of vector space, linear maps , span and linear independence. Using only abstract definitions ( with out the need for geometric vectors in \\(R^{n}\\) ), We realized that every vector space is characterized by a set of basis vectors. It turns out that these basis vectors suffice to succintly represent the entire vector space, due to the following property.\n\nUniqueness: Let \\(V\\) be a vector space and B be a set of basis vectors. Then every vector in \\(V\\) can be uniquely represented as a linear combination of vectors in B. Let us call the coefficients/scalar multiples Coordinates.\n\nProof sketch: Write down the two different representations of a vector in \\(V\\). Subtract the two and see that since the basis set is independent, we will have that each of the scalar corresponding to each vector equal to zero. (something like (a1-c1)\\(\\hat b_1\\) + (a2-c2)\\(\\hat b_2\\) \\(\\dots\\) = 0, implying a1=c1,a2=c2 etc as \\(\\hat b_1\\),\\(\\hat b_2\\) etc are linearly independent.)\n\nFixed Count: For any vector space eventhough the specific set of basis vectors might vary, the number of vectors in the set remain same. Let’s call it the dimension.\n\nProof sketch: This statement can be seen more clearly if we try to convince us of following two facts: - Consider a basis set B with size n of the vector space \\(V\\). Any set of vectors smaller than n can’t span the space \\(V\\).\n\nSimilarly and set of vectors in \\(V\\), of size greater than n can’t be linearly independent.\n\nAbove statements can be seen clearly by just attempting to write down what they imply. See how it boils down to characterizing solutions to system of linear equations. Below I show for one case:\n\nAbove properties gives us an equivalence between vectors in \\(R^{n}\\) and generic vectors. As any vector \\(\\hat v\\) belonging to a vector space can be uniqely represented by a list of co-ordinates [c1,c2,..cn] \\(\\in\\) \\(R^{n}\\).\nExample:\nFind the coordinate vector of \\(p(x) = 4 - x + 3x^2\\)with respect to the basis \\(\\{x^2, x, 1\\}\\) ?\nIt’s just \\((3,-1,4) \\in R^{3}\\)"
  },
  {
    "objectID": "posts/Linear-Algebra/index.html#idea-2-linear-maps-can-be-represented-as-a-matrix.",
    "href": "posts/Linear-Algebra/index.html#idea-2-linear-maps-can-be-represented-as-a-matrix.",
    "title": "Linear Algebra: 0.1",
    "section": "Idea 2 : Linear Maps can be represented as a Matrix.",
    "text": "Idea 2 : Linear Maps can be represented as a Matrix.\nWe learned that vector spaces can be succintly represented by the corresponding basis set. Now linear Maps(defined in previous section) between vector spaces can be represented by a matrix. Once we are given the basis for a vector-space any vector in the space is defined by the corresponding scalar multiples. Take any vector in \\(R^2\\), \\(\\hat a = (x,y)\\). It can be written as \\(\\hat a = x\\hat e_1 + y \\hat e_2\\), where \\(\\hat e_1 = (1,0),\\hat e_2 = (0,1)\\) are the vectors corresponding to co-ordinate axis(basis set). Also note how changing basis set changes the representation. Below I show how a matrix is just a convenient way of representing a linear mapping.\n\n\n\nImportant: Note how the structure of the linear map(linearity) and the vector-space are crucial for this succint representation to be possible.\n\nThis immediately directs us how best to define operations with matrix. Given a vector and a matrix corresponding to the linear map, their multiplication should give the resultant vector after applying the map. Similarly,matrix multiplication with another.\n\n\nImportant: Take time to internalize this. Typical definition of matrix multiplication might have seemed arbitrary but realizing that it is equivalent to the above might finally make it click. Also Matrix representation doesn’t make sense without specifying the corresponding basis-set. When not given we assume standard basis.(\\(\\hat e_1,\\hat e_2\\) etc)\n\n\nImportant: Before Proceeding Solve this toy problem (solution at the end)"
  },
  {
    "objectID": "posts/Linear-Algebra/index.html#idea-3-change-of-basis",
    "href": "posts/Linear-Algebra/index.html#idea-3-change-of-basis",
    "title": "Linear Algebra: 0.1",
    "section": "Idea 3 : Change of Basis",
    "text": "Idea 3 : Change of Basis\nBelow I show how to get the representation(matrix) corresponding to a linear map when we know it for one basis and asked to get it in another basis. Here we used the symbol \\(P^{-1}\\) to represent the inverse of a mapping. We will later discuss how to derive this from first principles,but the typical highschool algorithm involving determinants or gaussian elimination should suffice at the moment.\nWe already understand that a matrix vector multiplication is nothing but representing the given vector in different bases. To ensure we are completely clear on this, let’s do a sample problem.\n\nNow let’s see the effect of a linear map under the new basis.\n\n\n\nLet’s walkthrough what we have done above again. - We realized that a matrix represents a linear map and vice-versa. - Give points in the basis \\(v_1\\),\\(v_2\\) we can find a linear map that finds new coordinates of those points with the basis \\(e_1,e_2\\). We know how to get this matrix by looking at the effect of the linear map \\(P\\) on the basis set \\(v_1\\),\\(v_2\\).(check the note on matrix multiplication,if this is not clear). - Now the trick is to get \\(B\\), we can change the basis to \\(e_1,e_2\\) and use the representation of the map under this basis - \\(A\\) and then convert back to new basis,(round tour) to get \\(B\\).\nBefor we proceed, let’s understand linear maps,this would make our study of matrix decompositions more natural.\nLinear Maps - Properties\nWe already know the definition of a linear map and the fact that it can be represented by a matrix. And we have also seen how the change of basis effects a given linear map. But why change basis ? How does these ideas help solve problems ? Let’s gain insight by solving some problems.\nQ1. What is the standard matrix for the transpose map of a 2*2 matrix w.r.t standard basis ?\nQ2. Find the standard matrix of the differentiation map D : P 3 → P3 with respect to the standard basis {1, x, x2 , x3} ⊂ P3\nQ3. compute the fourth derivative of \\(x^2e^x + 2xe^x\\).\nBelow We will see how changing basis is useful sometimes. The key is to see that matrix multiplication is easy for diagonal matrices. Thus if a give linear transformation be converted be done in a basis such that the corresponding matrix is diagnol - we can recursively apply that transformation easily. Will be clear with below examples.\nQ4. Find 10th fibonacci number ?\nQ5. What is the standard matrix for the transpose map of a 2*2 matrix w.r.t Pauli’s basis ?\nQ6. Compute \\(\\int x^2e^{3x} dx\\) using matrices\nExtras\nTo further understand the structure of the vector space, below I share some other fun ideas. Some of these are a bit hard to understand and not strictly necessary. You can safely skip."
  },
  {
    "objectID": "posts/collected-thoughts-2/index.html",
    "href": "posts/collected-thoughts-2/index.html",
    "title": "Think on these #2",
    "section": "",
    "text": "There is a subtle difference between intention and expectation. We can have intention without any expectation. Intention comes without any bondage. Expectation pulls us into unnecessary drama – our life will depend on the currents of situations, drawing out life energies. Through intent and purpose, we express ourselves. Life is nothing but an expression of energy."
  },
  {
    "objectID": "posts/collected-thoughts-2/index.html#intention-vs-expectation",
    "href": "posts/collected-thoughts-2/index.html#intention-vs-expectation",
    "title": "Think on these #2",
    "section": "",
    "text": "There is a subtle difference between intention and expectation. We can have intention without any expectation. Intention comes without any bondage. Expectation pulls us into unnecessary drama – our life will depend on the currents of situations, drawing out life energies. Through intent and purpose, we express ourselves. Life is nothing but an expression of energy."
  },
  {
    "objectID": "posts/collected-thoughts-2/index.html#well-being",
    "href": "posts/collected-thoughts-2/index.html#well-being",
    "title": "Think on these #2",
    "section": "Well-being",
    "text": "Well-being\nSeeking acknowledgment and validation from others creates psychological dependency. Longing to be around people arises out of this compulsion to validate. We seek to be cared, respected, included. These primordial drives have kept us alive. All compulsions seem to arise out of a center and work to keep it alive. This is a live process that is happening beneath our awareness. Raising awareness towards this process brings well-being."
  },
  {
    "objectID": "posts/collected-thoughts-2/index.html#beyond-chatter",
    "href": "posts/collected-thoughts-2/index.html#beyond-chatter",
    "title": "Think on these #2",
    "section": "Beyond chatter",
    "text": "Beyond chatter\nWe have incredibly complex minds. Capturing it in a Self-image or ego is futile. Ever refreshing attention to the present is all that’s needed to capture the enormity of life. Ego constricts.Beyond chatter, there is a profound experience."
  },
  {
    "objectID": "posts/collected-thoughts-2/index.html#inner-exploration",
    "href": "posts/collected-thoughts-2/index.html#inner-exploration",
    "title": "Think on these #2",
    "section": "Inner Exploration",
    "text": "Inner Exploration\nWe are our minds.All that we call life arises out of it. Most of the time, our conscious brain runs on auto-pilot – compulsively rolling from thought to thought. Paradoxically, without any force but by just observing these thoughts and sensations as they arise in consciousness seems to have an immediate calming effect. The experience of paying attention to the contents of consciousness brings about fundamental changes in the very perception of life. It would be a disaster to go through life without ever exploring the depths of it."
  },
  {
    "objectID": "posts/collected-thoughts-2/index.html#experience",
    "href": "posts/collected-thoughts-2/index.html#experience",
    "title": "Think on these #2",
    "section": "Experience",
    "text": "Experience\nSeek experiences. Personal transformation arises under experiences bereft of any thought or rationalization. These leave a deep impression on us that makes change inevitable. If you fantasize a life of leisure and travel – take a small vacation, Curious about spirituality – take a day off and meditate. We are capable of following irrational group norms based on beliefs and ideals alone. I would rather explore life grounded in reality than led by the figments of cultural imagination. Poke around your boundaries, seek experiences."
  },
  {
    "objectID": "posts/collected-thoughts-2/index.html#on-impermanence",
    "href": "posts/collected-thoughts-2/index.html#on-impermanence",
    "title": "Think on these #2",
    "section": "On Impermanence",
    "text": "On Impermanence\nThoughts, feelings, emotions, beliefs change continuously. An appreciation of this fact is needed for a balanced life. Almost nothing of me remained from a decade ago…the contents of my consciousness and body have changed completely. But, there is an element in me that stitches a sense of continuity. The grip of the ephemeral loosens when one becomes aware. Not believe or theorize."
  },
  {
    "objectID": "posts/collected-thoughts-2/index.html#on-happiness",
    "href": "posts/collected-thoughts-2/index.html#on-happiness",
    "title": "Think on these #2",
    "section": "On happiness",
    "text": "On happiness\nIt all starts with doubting human condition. Why is it that happiness comes after fulfilling a desire and lasts only for a while? So we are bound to hop from a desire to desire, accumulating whatever angst along the way. Can there be joy untethered to desire ? If not, then a sensible solution would be to have easily achievable goals. This makes the whole hopping thing seem more continuous stream of happiness. But, do we have control over our desires ?"
  },
  {
    "objectID": "posts/collected-thoughts-2/index.html#on-love",
    "href": "posts/collected-thoughts-2/index.html#on-love",
    "title": "Think on these #2",
    "section": "On Love",
    "text": "On Love\nWe all seem to share a longing to expand. To become more. This drive manifests as a desire for wealth, status, recognition or freedom from all these. Our minds are not satisfied with the boundaries of our physical bodies – there is an element in us that is seeking to be boundless. What is stopping us from loving unconditionally ?"
  },
  {
    "objectID": "posts/collected-thoughts-2/index.html#on-education",
    "href": "posts/collected-thoughts-2/index.html#on-education",
    "title": "Think on these #2",
    "section": "On Education",
    "text": "On Education\nEducation can be better. A complete human being unsmitten by the relentless need to compare, capable of love is a rarity. When a child passes through a decade or so of education – his capacity towards life should be enhanced. Awareness brings about inner harmony that resolves the need for religion, government, authority, morality. Raising the awareness of our species is the best engineering solution against war, crime, pollution, and self-induced suffering."
  },
  {
    "objectID": "posts/collected-thoughts-2/index.html#stretch-your-limits",
    "href": "posts/collected-thoughts-2/index.html#stretch-your-limits",
    "title": "Think on these #2",
    "section": "Stretch your limits",
    "text": "Stretch your limits\nThought is limited to the past. It can’t operate without memory. And memory is limited and fallible. Belief is restricted thought. Collection of beliefs about oneself creates Ego. When a free thought disturbs the certainty of a belief – Ego shatters. Ego feeds on the stability of identity. Seeking to stretch your limits eventually renders Ego useless."
  },
  {
    "objectID": "posts/collected-thoughts-2/index.html#joyful-life",
    "href": "posts/collected-thoughts-2/index.html#joyful-life",
    "title": "Think on these #2",
    "section": "Joyful life",
    "text": "Joyful life\nWe are the creatures of limited time and energy. What we choose to make out of these is entirely our choice. Awareness of these basic constants leads to a responsible life. Complete responsibility equals absolute freedom. When we realize our ability to respond to life, events, thoughts will live a joyful life."
  },
  {
    "objectID": "posts/collected-thoughts-2/index.html#the-vicious-cycle",
    "href": "posts/collected-thoughts-2/index.html#the-vicious-cycle",
    "title": "Think on these #2",
    "section": "The Vicious cycle",
    "text": "The Vicious cycle\nWe Sense the world. And then we associate. We then segregate them into good and bad. Any further perception is limited by these associations."
  },
  {
    "objectID": "posts/collected-thoughts-2/index.html#discipline",
    "href": "posts/collected-thoughts-2/index.html#discipline",
    "title": "Think on these #2",
    "section": "Discipline",
    "text": "Discipline\nIt takes some effort to stick to a regimen. But it’s the only way to transform oneself. A sense of personal growth seems to be very fundamental for a good life. We all have this longing to expand and grow beyond ourselves. Whatever the means one may choose, discipline is the tool."
  },
  {
    "objectID": "posts/collected-thoughts-2/index.html#keep-it-simple",
    "href": "posts/collected-thoughts-2/index.html#keep-it-simple",
    "title": "Think on these #2",
    "section": "Keep it simple",
    "text": "Keep it simple\nLife is simple. Catch yourself, whenever you are trying to make it otherwise. Do not escape reality. Acknowledge your feelings and desires without judgment. Do not hurry. There is no compulsion to reach conclusions. Appreciate uncertainty. We can take better actions under this framework. Forcing reality is a loser’s game. Keep it simple."
  },
  {
    "objectID": "posts/collected-thoughts-2/index.html#pay-the-dues",
    "href": "posts/collected-thoughts-2/index.html#pay-the-dues",
    "title": "Think on these #2",
    "section": "Pay the dues",
    "text": "Pay the dues\nPay attention to your desires and fears. We are complex creatures. Often we are unsure of what we want. It takes deliberate struggle to evade dissatisfaction and boredom. So let it be. Pay the dues."
  },
  {
    "objectID": "posts/collected-thoughts-2/index.html#take-charge",
    "href": "posts/collected-thoughts-2/index.html#take-charge",
    "title": "Think on these #2",
    "section": "Take Charge",
    "text": "Take Charge\nImagine. Consciously, not impulsively imagine what your ideal life should be like. Imagine freely, the kind of family, the nature of work and the world you would like to live in.Feel it to the bone. Now, do What you should be doing today to move in that direction. You can either live intentionally or succumb to the compulsive desires sold to you. There is always somebody dictating you the norms, selling you their fears and insecurities. Taking charge involves paying enough attention to these superficial desires, so they lose their grip. To connect with your deepest longings, you need to imagine freely and act. The feedback can only come through action. So, it’s better to imagine yourself, take charge and act."
  },
  {
    "objectID": "posts/collected-thoughts-2/index.html#anger",
    "href": "posts/collected-thoughts-2/index.html#anger",
    "title": "Think on these #2",
    "section": "Anger",
    "text": "Anger\nIt’s absorbing. But, it damages both the ends. The physical sensations are not pleasurable,often the actor feels more pain than the recipient. A moment’s awareness settles everything down. Somewhere along the line,control is being dropped. It is a fun exercise to actively take note of these situations , and seek to involve in them. To non-judgmentally observe your response. It becomes impossible to be both conscious and angry."
  },
  {
    "objectID": "posts/collected-thoughts-3/index.html",
    "href": "posts/collected-thoughts-3/index.html",
    "title": "Think on these: On building things",
    "section": "",
    "text": "Start early. Wherever you are, now is the time. Those who made it might not tell you but luck is a big factor. The earlier you start, the more can you fail. It’s an odd’s game. Don’t be fooled by the myth of ‘Genius’ nor the ‘Hard work’. Cowards subscribe for ‘the genius’ while the winners and aspiring ones go for ‘the ‘Hard work’.Both are overly result-oriented. Planning for the worst and giving your all is the name of the game. And having time on your side lets you play it for a tad longer. Start now."
  },
  {
    "objectID": "posts/collected-thoughts-3/index.html#start-early-start-now",
    "href": "posts/collected-thoughts-3/index.html#start-early-start-now",
    "title": "Think on these: On building things",
    "section": "",
    "text": "Start early. Wherever you are, now is the time. Those who made it might not tell you but luck is a big factor. The earlier you start, the more can you fail. It’s an odd’s game. Don’t be fooled by the myth of ‘Genius’ nor the ‘Hard work’. Cowards subscribe for ‘the genius’ while the winners and aspiring ones go for ‘the ‘Hard work’.Both are overly result-oriented. Planning for the worst and giving your all is the name of the game. And having time on your side lets you play it for a tad longer. Start now."
  },
  {
    "objectID": "posts/collected-thoughts-3/index.html#intensity-at-will",
    "href": "posts/collected-thoughts-3/index.html#intensity-at-will",
    "title": "Think on these: On building things",
    "section": "Intensity, at will",
    "text": "Intensity, at will\nSpend some time with yourself. Remove all the distractions – thoughts, compulsions. Cultivating a craft where you can be completely absorbed, will open up a new dimension. When you remove distractions, life intensifies. This percolates into all areas of life. You will become capable of intensity, at will."
  },
  {
    "objectID": "posts/collected-thoughts-3/index.html#nobody-to-please",
    "href": "posts/collected-thoughts-3/index.html#nobody-to-please",
    "title": "Think on these: On building things",
    "section": "Nobody to please",
    "text": "Nobody to please\nLearn to disagree. Compulsively worrying to satisfy and please others at the stake of personal wellbeing is a pathological condition. Being spiteful and nasty is not an option. But, not negotiating for your well-being is pathetic. Over time, the effects percolate. Personal beliefs are altered to fit the reality. True capacities are shunned and never realized. Agreeable people are paid less. Their position in social hierarchy remains mediocre. There is no moral obligation to please anybody."
  },
  {
    "objectID": "posts/collected-thoughts-3/index.html#create-a-routine",
    "href": "posts/collected-thoughts-3/index.html#create-a-routine",
    "title": "Think on these: On building things",
    "section": "Create a routine",
    "text": "Create a routine\nCreate a routine. Else you will be put in one. Exercise your freedom to choose what you want to do. Repeating the same steps every day is the only way to gain mastery. It is by building this reservoir of work you can sustainably stick with it. This is your ticket to freedom. Uncommitted become the slaves of norms. They will always be struggling to escape, always at odds with reality. start with your body. Have an exercise routine that can be maintained. Next, Work. Choose what you want more of and create a routine around it. Do not be tempted to fit too much here. Slowly you will develop expertise and confidence that will let you do more of it."
  },
  {
    "objectID": "posts/collected-thoughts-3/index.html#the-most-important-thing",
    "href": "posts/collected-thoughts-3/index.html#the-most-important-thing",
    "title": "Think on these: On building things",
    "section": "The most important thing",
    "text": "The most important thing\nCultivate the ability to break-down complex problems into simpler ones. This is the meta-skill that translates into greatness in every field. Accomplishing ridiculously difficult tasks demands the ability to visualize the composing sub-tasks and formulate daily routines around them. On any day, there should always be one thing that takes your complete attention, irrespective of the breadth of the project. As an entrepreneur, your sole responsibility is to answer what is the most important thing, now ?"
  },
  {
    "objectID": "posts/collected-thoughts-3/index.html#on-work",
    "href": "posts/collected-thoughts-3/index.html#on-work",
    "title": "Think on these: On building things",
    "section": "On work",
    "text": "On work\nSelling time for money is addictive. It nullifies the odds of accumulating wealth. It makes one blind to possibilities, incapable of imaging the power of leverage. Aspiring entrepreneurs should never work for money. They should actively avoid fixed pay contracts. Only under the circumstance of receiving rare skill, knowledge can they transact their time. Actively seek mild discomfort. Once succumbed to the safety of a job, anything creative sounds dangerous and implausible. The closer you can be to the creative process, the better. For employment, Startups are the best bet. Manipulating your environment is the most direct way of influencing outcomes. Under no circumstance, your time should be spent away from the creative process. There are always better options."
  },
  {
    "objectID": "posts/collected-thoughts-3/index.html#luck",
    "href": "posts/collected-thoughts-3/index.html#luck",
    "title": "Think on these: On building things",
    "section": "Luck",
    "text": "Luck\nThere is a controllable element to what people refer to as Luck – being in the right place at the right time. Depending upon what you want to do, there are great places(people) around the world. Anything that can structurally avoid complacency, is a great choice. We should look for as much discomfort as possible and the most sensible way is to place yourself in a hard place with hard people."
  },
  {
    "objectID": "posts/collected-thoughts-3/index.html#consistency",
    "href": "posts/collected-thoughts-3/index.html#consistency",
    "title": "Think on these: On building things",
    "section": "Consistency",
    "text": "Consistency\nDon’t directly rush into doing something. Take some time and put some labor to find out why you want to create something. Only when you are clear on Why proceed further. The next important question is what you can do right now to get a bit closer? Daily commitments should be so simple that they sound laughably easy.This ensures momentum, as we achieve them daily. We are not naturally good at forecasting the cumulative effects of simple habits. The key to accomplishing ridiculously difficult tasks is consistency, not the quantum of work done on a day."
  },
  {
    "objectID": "posts/collected-thoughts-3/index.html#progress",
    "href": "posts/collected-thoughts-3/index.html#progress",
    "title": "Think on these: On building things",
    "section": "Progress",
    "text": "Progress\nClarity of what you want in the long term is absolutely crucial. Define it. Take some time and write it down. Yes, it might evolve. But you should have a clear understanding of the essence. Ask specific questions like How do you want your day to be? How does your ideal work environment look like? How does your relationships, health, financials look like? Create a practice to develop deeper understanding of your longings? Only with clarity of vision can there be a possibility of progress."
  },
  {
    "objectID": "posts/collected-thoughts-3/index.html#hard-work",
    "href": "posts/collected-thoughts-3/index.html#hard-work",
    "title": "Think on these: On building things",
    "section": "Hard Work",
    "text": "Hard Work\nWorking more hours is not hard work. Putting your heart to create something is not hard work. Constantly stretching your capabilities is not hard work. Taking responsibility is not hard work. Working for the paycheck, bearing and accepting the inanity of your work, waiting for the weekend, passing through life without intensity is Hard Work."
  },
  {
    "objectID": "posts/collected-thoughts-3/index.html#every-day",
    "href": "posts/collected-thoughts-3/index.html#every-day",
    "title": "Think on these: On building things",
    "section": "Every Day",
    "text": "Every Day\nFocus on what you are doing every day. Start with the smallest change that can be incorporated into your routine. Now stick to it every day. This is the secret of accomplishing impossibly difficult tasks. Focus on the routines and habits. Nothing more is needed."
  },
  {
    "objectID": "posts/collected-thoughts-3/index.html#fear-is-good",
    "href": "posts/collected-thoughts-3/index.html#fear-is-good",
    "title": "Think on these: On building things",
    "section": "Fear is good",
    "text": "Fear is good\nWhen you are starting something new, the fear of failure is often useful. This forces you to focus on risk. Being able to use this fear to ask the right questions, to strategies and come up with novel ideas is the defining skill of a successful entrepreneur. In the right proportions, it illuminates threats, directs the energies. Fear is good."
  },
  {
    "objectID": "posts/collected-thoughts-3/index.html#copying-ideas",
    "href": "posts/collected-thoughts-3/index.html#copying-ideas",
    "title": "Think on these: On building things",
    "section": "Copying Ideas",
    "text": "Copying Ideas\nDon’t get lost in the “next big idea” fallacy. There are two practical ways to come up with successful business ideas. Look for the problems you are facing or Copy from the solutions other entrepreneurs successfully implemented. The first category involves listing down all the activities in your day while thinking how each one of them can be made better. Now, once you choose a few, write down how much you are willing to pay and the kind of service or the product you want. If an idea filters through this process successfully, it can be considered further. Don’t fool yourself here,if you aren’t willing to pay chances are nobody will. The second category is much simpler. Choose an industry. (better you already got some idea about it.).Now find out what’s happening within it in other countries. Read their magazines, news sites, etc. Now the question boils down to, What successful business products and models can you take inspiration from? Yes, that’s called copying ideas. Hopefully, you are doing all this to create some value, not justify your genius."
  },
  {
    "objectID": "posts/collected-thoughts-3/index.html#little-by-little",
    "href": "posts/collected-thoughts-3/index.html#little-by-little",
    "title": "Think on these: On building things",
    "section": "Little by little",
    "text": "Little by little\nWe lack the ability to foresee the effects of small changes accumulated over time. Habits are remarkable ways to leverage the power of compounding. Introspect how your daily time is spent. Add or substitute a habit at a time. There is no hurry. This is a long term game. Keep the changes to absolute minimum.In the initial stages, momentum is the key. Whatever your ideal time or effort, reduce it by half – this should be your initial commitment. When creating habits, setting ridiculously easy goals so that it’s impossible to fail is the key. Consider any extra effort put as the bonus and feel great about it. Little by little your life will be transformed."
  },
  {
    "objectID": "posts/collected-thoughts-3/index.html#attention",
    "href": "posts/collected-thoughts-3/index.html#attention",
    "title": "Think on these: On building things",
    "section": "Attention",
    "text": "Attention\nAttention is limited. Any practice or technique that allows you to direct your energies will have a compounding effect on your life. Measure your day by how much of it is intentional. There are always people and things constantly attempting to grab your attention – the News, Internet, fancy toys. It’s a skill that can be trained."
  },
  {
    "objectID": "posts/DeepRl-prac/index.html",
    "href": "posts/DeepRl-prac/index.html",
    "title": "Deep Reinforcement Learning - Practice",
    "section": "",
    "text": "#hide\nfrom IPython.display import Image, display\ndest = '/Users/vinay/fastrl/images/'"
  },
  {
    "objectID": "posts/DeepRl-prac/index.html#introduction",
    "href": "posts/DeepRl-prac/index.html#introduction",
    "title": "Deep Reinforcement Learning - Practice",
    "section": "Introduction",
    "text": "Introduction\nLet’s start with humans learning from experience.Say we are trying to learn to bicycle. We are driven by a goal to stay balanced and pedal.Along the way we fall.Now we need to start again.Somewhere in the gap between each iteration,we are learning and improving.Let’s hash out the features of this process,if we are to model this :\n\nPast actions influence future output: There is no immediate feedback.Each micro-action(exerting more pressure on the pedal,..) along the way either leads to falling-off balance or keep moving.\n\nComputational Problem : How to assign credit to actions when they are not temporally connected ?\n\nOutcomes might not be deterministic: There are features of the environment(road,weather etc) that we do not fully understand that can effect outcome of the action.\n\nComputational Problem: How to make inference about the properties of a system under uncertainity ?\nSince out of the above two the latter seems to be simpler,let’s start by building our intuition about Non-deterministic systems : Let’s say we are given a Non-deterministic system whose properties we are unaware of but can only query it.How can we build our knowledge about the properties of this system ? Can we come up with a systematic way(algorithm) to estimate it’s properties ?\n\n#hide\nimport matplotlib.pyplot as plt\nimport torch\ndef query(): return torch.randn(1).item()*10\n\n\n\nquery(),query(),query() #Fires different measurements each time.\n\n(1.830642819404602, -18.865480422973633, 0.9646076709032059)\n\n\nNow to understand about any of the properties of this ‘query’,we need to start with making some assumptions - say the numbers are coming from some unknown distribution with some stable(constant) mean. Let’s start estimating this.\n\nNote:\n\n\nHow will your estimate of this mean change with each query ?\nHow does your confidence on this mean change with increasing number of queries \\(n\\) ?\n\n\ndef estimate_mean(n):\n    \"\"\"Returns list of estimated means after each query repeated for n times.\"\"\"\n    list_n = [] # keep track of all the outputs from our slot machine\n    list_mean = [] # collect the means after every sample.\n    for i in range(n):\n        out = query()\n        list_n.append(out)\n        list_mean.append(sum(list_n)/len(list_n))\n    return list_mean   \n\nNow let’s say after each query we update our estimate of running mean.\n\nestimate_mean(10) # The list of estimates for n = 1...10\n\n[-3.2570073008537292,\n 2.4307329952716827,\n -3.6120274662971497,\n -6.871547028422356,\n -6.159279823303223,\n -2.986470659573873,\n -2.4990574217268398,\n -1.9037501001730561,\n -2.7559810421533055,\n -1.0134802050888538]\n\n\n\n#let's plot these means\nlist_means = estimate_mean(100)\nplt.hist(list_means)\n\n(array([ 1.,  0.,  1., 32., 59.,  4.,  2.,  0.,  0.,  1.]),\n array([-4.82607206, -3.65700102, -2.48792998, -1.31885894, -0.1497879 ,\n         1.01928314,  2.18835417,  3.35742521,  4.52649625,  5.69556729,\n         6.86463833]),\n &lt;a list of 10 Patch objects&gt;)\n\n\n\n\n\nWe can see that the calculated mean vary a lot but seem to be closer to \\(0\\) most often..But how does our estimate itself depend on \\(n\\)(number of queries) ?\n\n#let's measure the stability of our estimates by taking the difference of each successive estimates.\n\ndiff_means = [list_means[i]-list_means[i-1] for i in range(1,len(list_means))]\nplt.plot(diff_means)\n\n\n\n\nOur estimate of the mean don’t seem to change much after a while.This is interesting.This aligns with our intuition - with more samples, we can be more confident.We can even go about proclaiming that whatever the dynamics of the system it’s mean might be constant ?\n\nImportant: Remember this insight..for any system with stable mean we can be sure that after a while our estimate itself becomes stable."
  },
  {
    "objectID": "posts/DeepRl-prac/index.html#neural-nets-with-q-learning",
    "href": "posts/DeepRl-prac/index.html#neural-nets-with-q-learning",
    "title": "Deep Reinforcement Learning - Practice",
    "section": "Neural Nets with Q-LEARNING",
    "text": "Neural Nets with Q-LEARNING\nReference : Playing Atari with Deep Reinforcement Learning\nKey Questions :\n1.Can we use neural nets in RL setting where the observations are correlated and Non-stationary(non-iid) ?\nsolution : Experiance Replay\n\nContext\n\n#hide\npsuedo_code = Image(dest+'Screenshot 2020-02-21 at 12.28.01 PM.png')\n\nThe key algorithm introduced in the paper is summarized below but we need necessary context before naively implementing it. Let us start with our knowledge of using Deep Nets in Supervised setting, and try to formulate the problem in similar context :\nFrom each state the agent encounters we want the agent to output a probability distribution over all possible actions, which when sampled from results in optimal return. So, if we can have access to the labels(optimal actions) from each state we can easily create a loss function and proceed with SGD over all the states.\nHere’s a simple approach for bootstrapping the labels:\n\nPlay the episode multiple times.\nRecord the returns\nChoose the episodes with highest return. These might be the ones that the agent chanced upon good actions,so train on them.\n\n\nCross-Entropy method\nHere’s how the environment looks like:\n\nyoutube: https://youtu.be/Qk61tqRrj0E\n\n\nimport gym\nimport torch\nimport numpy as np\nenv_name = \"CartPole-v0\"\nenv = gym.make(env_name)\n\ndef moving_average(x, w):\n    return np.convolve(x, np.ones(w), 'valid') / w\n\nHere we will use gym’s cartpole environment. Let’s quickly check the average return under random actions.\n\n#collapse-show\ndef play_episode(env,obs_acts,sampler):\n    ob = env.reset()\n    done = False\n    rewards = 0\n    while not done:\n        acts = obs_acts(ob)\n        actn = sampler(acts)\n        next_obs, reward,done, _ = env.step(actn)\n        rewards += reward\n        ob = next_obs\n    return rewards\n    \n\nAverage return for 1000 episodes\n\nnp.mean([play_episode(env,lambda x : env.action_space.n,np.random.choice) for i in range(1000)])\n\n22.295\n\n\nNow that’s the average return of a policy taking random actions at each state. In previous section, we have seen how properties of any distributin can be estimated in a step-wise manner,with each interaction resulting in more accurate estimate. But, here the reward from the environment depends on our actions(In this case moving left and right) at each state,with each action resulting in a different mean reward. Our task is to learn this mean reward corresponding to each action at a given state.\n\n#collapse-show\nimport torch\nimport torch.nn as nn\n\nobs_size = env.observation_space.shape[0]\nhidden_size = 128\n\nn_actions = env.action_space.n\nnet = nn.Sequential(nn.Linear(obs_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, n_actions))\n\n\n# Bootsrapping with the cross-entropy loss of best episodes.\n\nfrom collections import namedtuple\nimport torch.optim as optim\n\n\nobs_size = env.observation_space.shape[0]\nhidden_size = 128\nn_actions = env.action_space.n\nsft_max = nn.Softmax(dim=1)\nnet = nn.Sequential(nn.Linear(obs_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, n_actions))\n\n\ndef train(net,data,loss,optmizer):\n    optimizer.zero_grad()\n    keys,labels = data\n    pred_vals = net(keys)\n    loss_v = loss(pred_vals,labels)\n    print(f'loss before update {loss_v.item()}')\n    loss_v.backward()\n    optimizer.step()\n    print(f'loss after update {loss(net(keys),labels).item()}')\n\ndef categorical_sampler():\n    def sampler(probs):\n        return np.random.choice(len(probs),p=probs)\n    return sampler\n\ndef play_episode(env,net,sampler):\n    \n    episode_step = namedtuple('episode_step',field_names=['obs','actn'])\n    lis_steps = []\n    episode = namedtuple('episode',field_names=['tot_return','lis_steps'])\n    ob = env.reset()\n    done = False\n    rewards = 0\n    while not done:\n        acts = sft_max(net(torch.FloatTensor([ob])))\n        #import pdb;pdb.set_trace()\n        actn = sampler(acts.data.numpy()[0])\n        lis_steps.append(episode_step(ob,actn))\n        next_obs, reward,done, _ = env.step(actn)\n        rewards += reward\n        ob = next_obs\n    return episode(rewards,lis_steps)\n    \n\n\n\nbatch_size = 20\npercentile = 75\n\nsampler = categorical_sampler()\ndef get_batch(env,net,batch_size,sampler):\n    batch = []\n    while len(batch) &lt; batch_size:\n        episode = play_episode(env,net,sampler)\n        batch.append(episode)\n    return batch\n\n\nbatch = get_batch(env,net,10,sampler)\nlis_returns = list(map(lambda x: x.tot_return,batch))\nnp.mean(lis_returns)\n\n27.0\n\n\nNow,let’s train\n\n#collapse-show\nmean_returns = []\nloss = nn.CrossEntropyLoss()\noptimizer = optim.Adam(params=net.parameters(),lr=0.01)\nwhile True:\n        batch = get_batch(env,net,10,sampler)\n        lis_returns = list(map(lambda x: x.tot_return,batch))\n        mean_return = np.mean(lis_returns)\n        mean_returns.append(mean_return)\n        ret_threshold = np.percentile(lis_returns,percentile)\n        obs = []\n        actns = []\n        filtered_episodes = filter(lambda x : x.tot_return &gt;= ret_threshold,batch)\n        if mean_return &gt; 199: \n            print('training complete')\n            break\n        else:\n            for episode in filtered_episodes:\n                obs.extend(list(map(lambda x: x.obs,episode.lis_steps)))\n                actns.extend(list(map(lambda x: x.actn,episode.lis_steps)))\n            print(f'{len(mean_returns)} mean return = {mean_return},threshold = {ret_threshold}')\n            train(net,(torch.FloatTensor(obs),torch.LongTensor(actns)),loss,optimizer)\n    \n    \nimport matplotlib.pyplot as plt\nplt.plot(moving_average(mean_returns,10))\n\n\n\n\nIt’s cool that a simple approach derived out of our intuition actually worked. Now let’s check the performance of the same algorithm on FrozenLake environment.\n\nenv_fl = gym.make('FrozenLake-v0')\n\nHome Work 1 :\nSolve the above environment with the above method. For details of the environment refer FrozenLake\nCrossEntropy Method : Why it Works ?\nDue to the frequent use of CrossEntropy method in Supervised learning setting we have not spent time dealing with it’s formulation. Here’s the necessary deep dive :"
  },
  {
    "objectID": "posts/DeepRl-prac/index.html#deep-q-learning",
    "href": "posts/DeepRl-prac/index.html#deep-q-learning",
    "title": "Deep Reinforcement Learning - Practice",
    "section": "Deep-Q Learning",
    "text": "Deep-Q Learning\n\nPrerequisites\n\n\nPsuedo-code\n\n#hide_input\ndisplay(psuedo_code)"
  },
  {
    "objectID": "posts/DeepRl-prac/index.html#code",
    "href": "posts/DeepRl-prac/index.html#code",
    "title": "Deep Reinforcement Learning - Practice",
    "section": "Code",
    "text": "Code\n\n#collapse-show\nfrom collections import deque\nbatch_size = 100\nbatch = deque([],batch_size)\nmin_batch_size = 30\nenv_name = \"CartPole-v0\"\nenv = gym.make(env_name)\nobs_size = env.observation_space.shape[0]\nhidden_size = 128\nn_actions = env.action_space.n\nepsilon = 0.2\n\nnet = nn.Sequential(nn.Linear(obs_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, n_actions))\n\n\ndef e_greedy_sampler(epsilon):\n    def sampler(qf):\n        nped_qf = qf.detach().numpy().squeeze()\n        if np.random.uniform(0,1) &lt;= epsilon:\n            return np.random.choice(len(nped_qf))\n        else:\n            return np.argmax(nped_qf)\n    return sampler\n\ndef get_pred_q(episode,gamma=0.99):\n    rewards = [step.r for step in episode]\n    pred_q = [step.qa for step in episode]\n    #appending '0'  for terminal state.\n    pred_q.append(np.array(0))\n    labels = []\n    for i in range(len(rewards)):\n        labels.append(rewards[i]+gamma*pred_q[i+1].max().item())\n    return labels\n\n\nclass QAgent():\n    def __init__(self,net):\n        self.net = net\n        \n    def act(self,ob,fn = lambda x: x):\n        return fn(self.net(ob))\n    \n    def train(self,data,optimizer,loss_type ='squared'):\n        q_actions = []\n        actions = []\n        labels = []\n        for step,label in data:\n            labels.append(label)\n            q_actions.append(step.qa)\n            actions.append([step.a])\n        q_tensor = torch.cat(q_actions,0)\n        a_tensor = torch.tensor(actions)\n        labels_tensor = torch.tensor(labels)\n        q_actions_taken = torch.gather(q_tensor,1,a_tensor)\n        if loss_type == 'squared':\n            loss = nn.MSELoss()\n        loss = loss(q_actions_taken.squeeze(-1),labels_tensor)\n        #print(f'loss:{loss}')\n        loss.backward(retain_graph=True)\n        optimizer.step()\n        \nagent = QAgent(net) # The same network we used in cross-entropy method.\n\n#change our previous play_episode to store (ob,a,q,r,next_ob)\ndef play_episode(agent,env,sampler,n_steps=1):\n    epi_step = namedtuple('epi_step',field_names=['ob','a','qa','r'])\n    episode = []\n    pred_q = []\n    tot_return = 0\n    ob = env.reset()\n    done = False\n    while not done:\n        qf = agent.act(torch.FloatTensor([ob]))\n        action = sampler(qf)\n        #import pdb;pdb.set_trace()\n        next_ob,r,done,_ = env.step(action)\n        tot_return += r\n        episode.append(epi_step(ob,action,qf,r))\n        ob = next_ob\n    pred_q = get_pred_q(episode)\n    return tot_return,[(episode_step,pred_q[i]) for i,episode_step in enumerate(episode)]\n\n\nsampler = e_greedy_sampler(epsilon)\noptimizer = optim.Adam(params=net.parameters(),lr=0.01)\nthreshold = 199\n\ndef train(sampler,optimizer,threshold,agent,env):\n    return_vals = []\n    counter = 0 \n    while True:\n        episodic_ret,lis_step_label = play_episode(agent,env,sampler,n_steps=1)\n        counter += 1\n        return_vals.append(episodic_ret)\n        #print(f'{counter} return == {episodic_ret}')\n        if episodic_ret &gt; threshold:\n            #print('solved.')\n            break\n        batch.extend(lis_step_label)\n        if len(batch) &gt; min_batch_size:\n            idx = np.random.choice(len(batch),min_batch_size)\n            optimizer.zero_grad()\n            loss = agent.train([batch[id] for id in idx],optimizer,loss_type='squared')\n    return return_vals\n\nreturn_vals = train(sampler,optimizer,threshold,agent,env)\n        \n\n\nplt.plot(return_vals)\n\n\n\n\nThat took about 700 episodes and there’s almost no general trend.Let’s see if could replicate :\n\n# Run this when you got lot of free time.\nlis_return_vals = []\nfor i in range(20):\n    net = nn.Sequential(nn.Linear(obs_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, n_actions))\n    agent = QAgent(net)\n    lis_return_vals.append(train(sampler,optimizer,threshold,agent,env))\n\n\n#but here's a sample plot :\nplt.plot(lis_return_vals[0])\n\n\n\n\nHome Work 2 :\nThough we seem to have solved the environment,the training(episodic rewards) process seems too unstable. 1. Can you make it stable ? 2. List some of the possible reasons for instability ?\n\nBag of Tricks :\n\nTip: SOME OBVIOUS TECHNIQUES\n\n\nIn our implementation of Deep-Q learning agent we have kept epsilon constant throughout the training. We can anneal it towards a lower value as the training proceeds.\nOur update was Q(i) = r(i) + gammaQ(i+1) . So after every batch of episodes(after the update) we would expect our Q(i)’s closer to the labels (r(i) + gammaQ(i+1)) but due to the proximity of Q(i) and Q(i+1) updating Q(i) also changes Q(i+1). Unlike supervised setting, our labels here are a not constant,thus introducing instability in training.We can keep our labels constant for a couple of batches ( by storing the weights and using them to produce labels).\n\nWell, DeepMind pushed this a whole lot further by adding several other techniques in this paper :\nRainbow: Combining Improvements in Deep Reinforcement Learning"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Deep Learning - Quick Revision\n\n\n\n\n\n\n\nml\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2023\n\n\nVinay Varma\n\n\n\n\n\n\n  \n\n\n\n\nExperimentation in the Wild : Tools\n\n\n\n\n\n\n\nml\n\n\ncausal-inference\n\n\n\n\n\n\n\n\n\n\n\nJun 4, 2023\n\n\nVinay Varma\n\n\n\n\n\n\n  \n\n\n\n\nLinear Algebra: 0.1\n\n\n\n\n\n\n\nmaths-for-ml\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2022\n\n\nVinay Varma\n\n\n\n\n\n\n  \n\n\n\n\nThink on these: On building things\n\n\n\n\n\n\n\nlife\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2021\n\n\nVinay Varma\n\n\n\n\n\n\n  \n\n\n\n\nThink on these #2\n\n\n\n\n\n\n\nlife\n\n\n\n\n\n\n\n\n\n\n\nMay 18, 2021\n\n\nVinay Varma\n\n\n\n\n\n\n  \n\n\n\n\nDeep Reinforcement Learning - Theory\n\n\n\n\n\n\n\nml\n\n\nrl\n\n\n\n\n\n\n\n\n\n\n\nFeb 21, 2021\n\n\nVinay Varma\n\n\n\n\n\n\n  \n\n\n\n\nDeep Reinforcement Learning - Practice\n\n\n\n\n\n\n\nml\n\n\nrl\n\n\n\n\n\n\n\n\n\n\n\nFeb 21, 2021\n\n\nVinay Varma\n\n\n\n\n\n\n  \n\n\n\n\nThink on these #1\n\n\n\n\n\n\n\nlife\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2020\n\n\nVinay Varma\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is Vinay Varma. Here are my ramblings on life and A.I."
  }
]